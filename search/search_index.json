{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Deep Learning Toys","text":""},{"location":"#contents","title":"Contents","text":"<ul> <li>U-Net: U-Net for image segmentation.</li> <li>Denoising Diffusion Probabilistic Model: DDPM for (MNIST) image generation.</li> </ul>"},{"location":"DDPM/","title":"DDPM","text":""},{"location":"DDPM/#denoising-diffusion-probabilistic-model","title":"Denoising Diffusion Probabilistic Model","text":""},{"location":"DDPM/#ddpm","title":"\u6269\u6563\u6a21\u578b\u4e0e DDPM","text":"<p>\u6269\u6563\u6a21\u578b\u4ee5\u5176\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u751f\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u9ad8\u8d28\u91cf\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u8fc5\u901f\u6210\u4e3a\u76ee\u524d\u7684\u751f\u6210\u5f0f\u6a21\u578b\u524d\u6cbf\u3002DDPM (Denoising Diffusion Probabilistic Model) \u662f\u6700\u57fa\u672c\u7684\u6269\u6563\u6a21\u578b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p>\u8bbe\u539f\u59cb\u7684\u771f\u5b9e\u56fe\u50cf\u4e3a \\(x_0\\)\uff0c\u5206 \\(T\\) \u6b65\u5bf9\u5176\u4f9d\u6b21\u52a0\u5355\u4f4d\u9ad8\u65af\u566a\u58f0 (\\(\\mathcal{N}(0, I)\\))\uff0c\u53ef\u4ee5\u8fd1\u4f3c\u8ba4\u4e3a\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u7eaf\u9ad8\u65af\u566a\u58f0\u3002\u5982\u679c\u80fd\u591f\u9884\u6d4b\u51fa\u6bcf\u6b65\u6dfb\u52a0\u7684\u566a\u58f0\uff0c\u90a3\u4e48\u5c06\u53ef\u4ee5\u4ece\u4e00\u4e2a\u7eaf\u9ad8\u65af\u566a\u58f0\u9010\u6b65\u53bb\u566a\u751f\u6210\u4e00\u5f20\u65b0\u7684\u771f\u5b9e\u56fe\u50cf\u3002</p>"},{"location":"DDPM/#dataset-mnist","title":"Dataset: MNIST","text":"<p>MNIST \u6570\u636e\u96c6 (Mixed National Institute of Standards and Technology database) \u662f\u7f8e\u56fd\u56fd\u5bb6\u6807\u51c6\u4e0e\u6280\u672f\u7814\u7a76\u9662\u6536\u96c6\u6574\u7406\u7684\u5927\u578b\u624b\u5199\u6570\u5b57\u6570\u636e\u5e93\uff0c\u5305\u542b 60,000 \u4e2a\u793a\u4f8b\u7684\u8bad\u7ec3\u96c6\u4ee5\u53ca 10,000 \u4e2a\u793a\u4f8b\u7684\u6d4b\u8bd5\u96c6\u3002</p> <p>\u4e00\u822c\u7ed9\u51fa\u7684 MNIST \u6570\u636e\u96c6\u4e0b\u8f7d\u94fe\u63a5\u4e3a http://yann.lecun.com/exdb/mnist/index.html\uff0c\u7136\u800c\u76ee\u524d\u9700\u8981\u767b\u5f55\u9a8c\u8bc1\uff0c\u56e0\u6b64\u4f7f\u7528 <code>torchvision.datasets</code> \u7684\u65b9\u6cd5\u51c6\u5907\u8be5\u6570\u636e\u96c6\u3002</p> <p>\u4e0d\u540c\u4e8e\u5e38\u89c1\u7684\u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u4f7f\u7528 LeNet-5 \u5728 MNIST \u4e0a\u8fdb\u884c\u5206\u7c7b\uff0c\u672c\u5b9e\u9a8c\u5c06\u57fa\u4e8e DDPM \u5efa\u6a21 MNIST \u624b\u5199\u6570\u5b57\u7684\u6570\u636e\u5206\u5e03\uff0c\u4ece\u800c\u80fd\u591f\u91c7\u6837\u751f\u6210\u65b0\u7684\u624b\u5199\u6570\u5b57\u56fe\u7247\u3002</p>"},{"location":"DDPM/#preliminaries","title":"Preliminaries","text":"<p>\u8bf7\u81ea\u884c\u5b66\u4e60\u6269\u6563\u6a21\u578b\u7684\u57fa\u7840\u7406\u8bba\uff0c\u53ef\u4ee5\u53c2\u8003\u7684\u8d44\u6599\u6709</p> <ul> <li>Lilian Weng \u7684\u535a\u5ba2\uff1aWhat are Diffusion Models?</li> <li>\u6700\u521d\u7684 DDPM \u8bba\u6587\uff1aDenoising Diffusion Probabilistic Models</li> <li>DDIM \u8bba\u6587\uff1aDenoising Diffusion Implicit Models</li> </ul>"},{"location":"DDPM/#important-formula","title":"Important Formula","text":"<p>\u5728\u6b64\u76f4\u63a5\u7ed9\u51fa\u4e00\u4e9b\u91cd\u8981\u7ed3\u8bba\uff1a</p> <ul> <li>\u52a0\u566a\u516c\u5f0f\uff1a\\(x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_{t-1} + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\varepsilon\\)</li> </ul> <p>\u5bf9\u4e8e\u8fd9\u91cc\u7684\u7b26\u53f7\uff0c\u56fa\u5b9a\u52a0\u566a\u8fc7\u7a0b\u4f1a\u9884\u5148\u6307\u5b9a \\(\\beta_t\\) \u5982\u4e0b\uff1a</p> \\[ q(x_t|x_{t-1})=\\mathcal{N}(\\sqrt{1-\\beta_t}x_{t-1},\\beta_tI) \\] <p>\\(\\alpha_t = 1 - \\beta_t\\) \u662f\u5173\u4e8e \\(t\\) \u5355\u8c03\u9012\u51cf\u7684\uff0c\\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\)\u3002\\(\\varepsilon\\sim \\mathcal{N}(0, I)\\)\u3002</p> <ul> <li>DDPM \u53bb\u566a\u516c\u5f0f\uff1a\\(x_{t-1}\\) \u4ece \\(q(x_{t-1}|x_t)\\) \u4e2d\u91c7\u6837\uff0c\u8be5\u5206\u5e03\u8fd1\u4f3c\u4e3a\u9ad8\u65af\u5206\u5e03 \\(\\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_tI)\\)\uff0c\u5176\u4e2d</li> </ul> \\[ \\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\varepsilon_ {\\theta}^{(t)}(x_ t)\\right), \\quad \\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t \\] <ul> <li>DDIM \u53bb\u566a\u516c\u5f0f\uff1a</li> </ul> \\[     x_{t-1} = \\sqrt{\\bar{\\alpha}_ {t-1}}     \\underbrace{\\left( \\frac{x_t-\\sqrt{1-\\bar{\\alpha}_ t}\\varepsilon_ {\\theta}^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_ t}} \\right)}_ {\\text{\u201c predicted }x_0\\text{\u201d}}     + \\underbrace{\\sqrt{1-\\bar{\\alpha}_ {t-1}-\\sigma_ t^2}\\varepsilon_ {\\theta}^{(t)}(x_ t)}_ {\\text{\u201cdirection pointing to }x_t\\text{\u201d}}     + \\underbrace{\\sigma_ t \\varepsilon_ t}_ {\\text{random noise}} \\] <p>\\(\\varepsilon_ {\\theta}^{(t)}(x_ t)\\) \u662f\u6a21\u578b\u6839\u636e \\(x_t\\) \u9884\u6d4b\u51fa\u7684\u566a\u58f0\uff0c\\(\\varepsilon_ t\\) \u662f\u968f\u673a\u91c7\u6837\u7684\u5355\u4f4d\u9ad8\u65af\u566a\u58f0</p> <p>\u5176\u4e2d \\(\\sigma_t^2=\\eta \\tilde{\\beta}_t\\) \u662f\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u65b9\u5dee\uff0c\u6ce8\u610f\u5230\u6709</p> <ul> <li>\\(\\eta = 1\\) \u65f6\uff0c\u5c31\u7b49\u4ef7\u4e8e DDPM \u7684\u53bb\u566a\u516c\u5f0f</li> <li>\\(\\eta = 0\\) \u65f6\uff0c\u5c31\u662f DDIM \u7684\u53bb\u566a\u516c\u5f0f\uff0c\u4e0d\u786e\u5b9a\u9879 \\(\\varepsilon_ t\\) \u4e0d\u518d\u5b58\u5728</li> <li>\\(\\eta \\in (0, 1)\\) \u65e2\u4e0d\u662f DDPM \u4e5f\u4e0d\u662f DDIM</li> </ul> <p>\u6ce8\u610f\u53bb\u566a\u516c\u5f0f\u4e2d\u7684 \\(t-1\\) \u53ef\u4ee5\u53d8\u6210 \\(t-k\\)\uff0c\u5728\u9700\u8981\u52a0\u901f\u63a8\u65ad\u65f6\u3002</p>"},{"location":"DDPM/#training","title":"Training","text":"<p>\u5b9e\u9645\u8bad\u7ec3\u65f6\uff0c\u6211\u4eec\u968f\u673a\u751f\u6210\u4e00\u7cfb\u5217\u65f6\u95f4 \\(t\\)\uff0c\u6839\u636e\u8fd9\u4e9b \\(t\\) \u751f\u6210\u968f\u673a\u5355\u4f4d\u9ad8\u65af\u566a\u58f0\u5bf9\u6570\u636e \\(x_0\\) \u8fdb\u884c\u52a0\u566a\u5f97\u5230 \\(x_t\\) \uff0c\u7136\u540e\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u566a\u58f0 \\(\\varepsilon_ {\\theta}^{(t)}(x_ t)\\)\uff0c\u4e0e\u566a\u58f0\u771f\u503c\u8ba1\u7b97 MSE \u635f\u5931\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u3002</p> <p>\u7136\u800c\uff0c\u6839\u636e\u8bba\u6587 Progressive Distillation for Fast Sampling of Diffusion Models\uff0c\u8ba9\u6a21\u578b\u9884\u6d4b\u4e00\u4e2a\u901f\u5ea6 \\(v\\) \u4f1a\u6bd4\u76f4\u63a5\u9884\u6d4b\u566a\u58f0 \\(\\varepsilon\\) \u5177\u6709\u66f4\u597d\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3002\u8bad\u7ec3\u65f6\uff0c\u9700\u8981\u6839\u636e\u539f\u59cb\u6570\u636e \\(x_0\\) \u548c\u566a\u58f0 \\(\\varepsilon\\) \u8ba1\u7b97\u51fa\u6a21\u578b\u6240\u9700\u8981\u9884\u6d4b\u7684\u901f\u5ea6 \\(v\\)\uff1a</p> \\[ v = \\sqrt{\\overline{\\alpha}_t} \\varepsilon - \\sqrt{1-\\overline{\\alpha}_t} x_0 \\] <p>\u7136\u540e\u548c\u6a21\u578b\u9884\u6d4b\u7684\u901f\u5ea6 \\(v_{\\theta}^{(t)}(x_ t)\\) \u8ba1\u7b97 MSE \u635f\u5931\u5373\u53ef\u3002</p> <p>\u5173\u4e8e v-prediction \u7684\u66f4\u591a\u5185\u5bb9\uff0c\u9664\u4e86\u539f\u8bba\u6587\u5916\uff0c\u4e5f\u53ef\u4ee5\u53c2\u8003\u672c\u4eba\u7684\u7b14\u8bb0</p>"},{"location":"DDPM/#inference","title":"Inference","text":"<p>\u4ece \\(x_T\\) \u5f00\u59cb\uff0c\u4e00\u6b65\u6b65\u53bb\u566a\u5f97\u5230 \\(x_0\\) \u5373\u53ef\u3002\u5bf9\u4e8e epsilon-prediction \u6a21\u578b\uff08\u76f4\u63a5\u9884\u6d4b\u566a\u58f0\uff09\uff0c\u76f4\u63a5\u4f7f\u7528\u524d\u9762\u7684\u53bb\u566a\u516c\u5f0f\u5373\u53ef\uff0c\u53ea\u9700\u8981\u5728\u9700\u8981\u52a0\u901f\u63a8\u65ad\u65f6\u589e\u5927\u53bb\u566a\u7684\u6b65\u957f\u3002</p> <p>\u5bf9\u4e8e v-prediction\uff0c\u5173\u6ce8 DDIM \u53bb\u566a\u516c\u5f0f\uff0c\u53ea\u9700\u8981\u4fee\u6539\u5176\u4e2d\u7684 predicted \\(x_0\\)\uff0c\u5373</p> \\[ x_0 = \\sqrt{\\overline{\\alpha}_t} x_t - \\sqrt{1-\\overline{\\alpha}_t} v \\]"},{"location":"DDPM/#tasks","title":"Tasks","text":"<ol> <li>\u8865\u5168 <code>scheduler.py</code> \u4e2d\u6240\u6709\u7684 TODO\uff0c\u4f7f\u5f97\u901a\u8fc7\u7ed9\u5b9a\u7684 <code>unet.py</code> \u548c <code>train.py</code> \u53ef\u4ee5\u8bad\u7ec3\u5f97\u5230 epsilon-prediction \u6216 v-prediction \u7684\u6a21\u578b</li> <li>\u81ea\u5df1\u5199\u4e00\u4e2a <code>infer.py</code>\uff0c\u80fd\u591f\u8f7d\u5165\u9884\u8bad\u7ec3\u7684 epsilon-prediction \u6216\u8005 v-prediction \u6a21\u578b\uff0c\u63a8\u65ad\u751f\u6210\u65b0\u7684\u624b\u5199\u6570\u5b57\u56fe\u7247</li> <li>\u7814\u7a76\u6a21\u578b\u9884\u6d4b\u7c7b\u578b\u3001\u63a8\u65ad\u6b65\u6570\u4ee5\u53ca DDIMScheduler \u7684 \\(\\eta\\) \u53c2\u6570\u5bf9\u751f\u6210\u7684\u5f71\u54cd</li> </ol>"},{"location":"DDPM/#download-links","title":"Download Links","text":"<p>\u672c\u5b9e\u9a8c\u6682\u65f6\u4e0d\u516c\u5f00\u5b8c\u6574\u5185\u5bb9\uff0c\u5982\u679c\u6709\u9700\u8981\u53ef\u4ee5\u8054\u7cfb ZhouTimeMachine</p> <ul> <li>train.py</li> <li>unet.py</li> <li>scheduler.py</li> </ul>"},{"location":"DDPM/#references","title":"References","text":"<ul> <li>MNIST dataset (verification is needed)</li> <li>Lilian Weng \u7684\u535a\u5ba2\uff1aWhat are Diffusion Models?</li> <li>\u6700\u521d\u7684 DDPM \u8bba\u6587\uff1aDenoising Diffusion Probabilistic Models</li> <li>DDIM \u8bba\u6587\uff1aDenoising Diffusion Implicit Models</li> <li>v-prediction \u8bba\u6587\uff1aProgressive Distillation for Fast Sampling of Diffusion Models</li> <li>\u672c\u4eba\u5173\u4e8e v-prediction \u7684\u7b14\u8bb0</li> </ul>"},{"location":"UNet/","title":"U-Net","text":""},{"location":"UNet/#u-net","title":"U-Net","text":"<p>Lab4 for ZJU 2023 Computer Vision course, link</p> <p>U-Net \u662f\u4e00\u4e2a\u7ecf\u5178\u7684\u8bed\u4e49\u5206\u5272\u5168\u5377\u79ef\u7f51\u7edc\uff0c\u6700\u521d\u5e94\u7528\u4e8e\u533b\u7597\u56fe\u50cf\u7684\u5206\u5272\u4efb\u52a1\u3002\u5176\u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u53ef\u4ee5\u770b\u5230 U-Net \u6709\u4e00\u4e2a\u5bf9\u79f0\u7684\u7ed3\u6784\uff0c\u5de6\u8fb9\u662f\u4e00\u4e2a\u5178\u578b\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u53f3\u8fb9\u662f\u4e00\u4e2a\u5bf9\u79f0\u7684\u4e0a\u91c7\u6837\u7f51\u7edc\uff0c\u53ef\u4ee5\u5c06\u5de6\u8fb9\u7684\u7279\u5f81\u56fe\u6062\u590d\u5230\u539f\u56fe\u5927\u5c0f\u3002</p> <p>\u66f4\u591a\u8be6\u7ec6\u5185\u5bb9\u53ef\u4ee5\u53c2\u8003 U-Net \u539f\u8bba\u6587 U-Net: Convolutional Networks for Biomedical Image Segmentation\u3002</p>"},{"location":"UNet/#dataset-carvana","title":"Dataset: Carvana","text":"<p>Carvana dataset \u662f kaggle \u4e0a\u7684\u4e00\u4e2a\u8bed\u4e49\u5206\u5272\u7ade\u8d5b\u6570\u636e\u96c6\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u5bf9\u6c7d\u8f66\u7684\u5206\u5272\u3002</p> <p>\u6839\u636e Carvana \u6570\u636e\u96c6\u7684\u5212\u5206\uff0c\u5176\u8bad\u7ec3\u96c6\u5305\u542b 5088 \u5f20\u6c7d\u8f66\u56fe\u7247 (.jpg) \u548c\u5bf9\u5e94\u7684\u63a9\u7801 (mask, .gif)\uff0c\u63a9\u7801\u53ef\u4ee5\u8ba4\u4e3a\u662f 0-1 \u7684\uff0c\u8868\u793a\u56fe\u7247\u4e0a\u6bcf\u4e2a\u50cf\u7d20\u662f\u5426\u5c5e\u4e8e\u6c7d\u8f66\u3002\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u5904\u7406\u6210\u9010\u50cf\u7d20\u7684\u4e8c\u5206\u7c7b\u95ee\u9898\u3002</p> <p>\u4e0d\u8981\u6c42\u4e0b\u8f7d\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u63d0\u4f9b\u5df2\u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002</p>"},{"location":"UNet/#u-net-completion-and-inference","title":"U-Net Completion and Inference","text":""},{"location":"UNet/#structure-of-u-net","title":"Structure of U-Net","text":"<p>\u5728 U-Net \u539f\u8bba\u6587 U-Net: Convolutional Networks for Biomedical Image Segmentation \u4e2d\uff1a</p> <ul> <li>\u5de6\u4fa7\u5411\u4e0b\u7684\u7ed3\u6784\u88ab\u79f0\u4e3a Contracting Path\uff0c\u7531\u901a\u9053\u6570\u4e0d\u65ad\u589e\u52a0\u7684\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\u7ec4\u6210</li> <li>\u53f3\u4fa7\u5411\u4e0a\u7684\u7ed3\u6784\u88ab\u79f0\u4e3a Expanding Path\uff0c\u7531\u901a\u9053\u6570\u4e0d\u65ad\u51cf\u5c11\u7684\u5377\u79ef\u5c42\u548c\u4e0a\u91c7\u6837\u5c42\uff08\u53cd\u5377\u79ef\u5c42\uff09\u7ec4\u6210</li> <li>\u5728 Expanding Path \u4e2d\uff0c\u6bcf\u6b21\u4e0a\u91c7\u6837\u5c42\u90fd\u4f1a\u5c06 Contracting Path \u4e2d\u5bf9\u5e94\u7684\u7279\u5f81\u56fe\u4e0e\u81ea\u8eab\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u62fc\u63a5\uff0c\u8fd9\u6837\u53ef\u4ee5\u4fdd\u8bc1 Expanding Path \u4e2d\u7684\u6bcf\u4e00\u5c42\u90fd\u80fd\u591f\u5229\u7528 Contracting Path \u4e2d\u7684\u4fe1\u606f</li> </ul>"},{"location":"UNet/#network-completion","title":"Network Completion","text":"<p>\u8981\u6c42\u5b8c\u6210 unet.py \u4e2d\u5168\u90e8\u7684 <code>TODO</code>\uff0c\u4f7f\u5f97\u6240\u63d0\u4f9b\u7684\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u53ef\u4ee5\u88ab\u6b63\u786e\u52a0\u8f7d\u3002</p> <p>\u6574\u4f53\u4e0a\u6765\u770b\uff0c\u9700\u8981\u5b8c\u6210\u7684\u5185\u5bb9\u4e3a <code>UNet</code> \u7c7b <code>__init__</code> \u4e2d\u90e8\u5206\u5377\u79ef\u5c42\u7684\u5b9a\u4e49\uff0c\u4ee5\u53ca <code>forward</code> \u51fd\u6570\u4e2d\u7684 Contracting Path \u548c Expanding Path \u7684\u524d\u9012\u3002\u4e0d\u8fc7\uff0c<code>UNet</code> \u7c7b <code>__init__</code> \u8865\u5168\u8fc7\u7a0b\u4e2d\u8fd8\u9700\u8981\u5b9e\u73b0 <code>CropAndConcat</code> \u7c7b\u3002</p> <p>\u4ee5\u4e0b\u4ee3\u7801\u53ea\u662f\u5c55\u793a\u4e00\u4e2a\u6846\u67b6\u4fbf\u4e8e\u7eb5\u89c2\u5168\u90e8\u5185\u5bb9\uff0c\u8bf7\u70b9\u51fb\u6587\u6863\u4e2d unet.py \u6587\u5b57\u6240\u5bf9\u5e94\u7684\u94fe\u63a5\u4e0b\u8f7d\u5305\u542b\u66f4\u591a\u63d0\u793a\u7684\u6a21\u677f\u8fdb\u884c\u7f51\u7edc\u8865\u5168</p> <pre><code>class UNet(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        ...\n\n        # TODO: Double convolution layers for the contracting path.\n        ...\n\n        # Down sampling layers for the contracting path\n\n        # TODO: The two convolution layers at the lowest resolution (the bottom of the U).\n\n        # Up sampling layers for the expansive path.\n\n        # TODO: Double convolution layers for the expansive path.\n        ...\n\n        # Crop and concatenate layers for the expansive path.\n        # TODO: Implement class CropAndConcat starting from line 6\n        ...\n\n        # TODO: Final 1*1 convolution layer to produce the output\n        ...\n\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: input image\n        \"\"\"\n        # TODO: Contracting path\n        ...\n\n        # Two 3*3 convolutional layers at the bottom of the U-Net\n        x = self.middle_conv(x)\n\n        # TODO: Expansive path\n        ...\n</code></pre>"},{"location":"UNet/#__init__-convolution-layers","title":"<code>__init__</code>: Convolution Layers","text":"<ul> <li>\u901a\u9053\u6570\u7684\u53d8\u5316\u5df2\u7ecf\u5728\u524d\u9762\u7684\u56fe\u4e2d\u8fdb\u884c\u4e86\u6807\u6ce8\u3002\u548c\u539f\u8bba\u6587\u4e0d\u540c\uff0c\u8bad\u7ec3\u65f6 final_conv \u7684\u8f93\u51fa\u901a\u9053\u6539\u6210\u4e86 1\uff0c\u4f46\u662f\u5728\u901a\u7528\u7684\u7f51\u7edc\u7ed3\u6784\u4e2d\u5c31\u662f <code>out_channels</code></li> <li>down_conv, mid_conv \u548c up_conv \u90fd\u662f\u7531\u4e24\u4e2a\u5377\u79ef\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e2a\u5377\u79ef\u5c42\u90fd\u662f \\(3\\times 3\\) \u7684\u5377\u79ef\u6838\uff0cpadding \u4e3a \\(1\\)\uff0cstride \u4e3a \\(1\\)\u3002\u6bcf\u4e2a\u5377\u79ef\u5c42\u540e\u90fd\u6709\u4e00\u4e2a ReLU \u6fc0\u6d3b\u51fd\u6570\uff0c\u6574\u4f53\u987a\u5e8f\u4e3a Conv2d-Relu-Conv2d-Relu</li> <li>final_conv \u662f\u4e00\u4e2a \\(1\\times 1\\) \u7684\u5377\u79ef\u5c42\uff0cpadding \u4e3a \\(0\\)\uff0cstride \u4e3a \\(1\\)\uff0c\u6ca1\u6709\u6fc0\u6d3b\u51fd\u6570</li> </ul> <p>\u53ea\u9700\u8981\u5728\u6709 <code>TODO</code> \u7684\u5730\u65b9\u586b\u5199\uff0c\u5728 <code>nn.Sequential</code> \u7684\u62ec\u53f7\u4e2d\u6b63\u5e38\u586b\u5199 <code>nn.Conv2d</code>, <code>nn.ReLU</code> \u5373\u53ef\u3002\u4e0d\u8981\u81ea\u5b9a\u4e49\u7f51\u7edc\u7c7b\uff0c\u907f\u514d\u6a21\u578b\u56e0\u4e3a\u5c42\u547d\u540d\u4e0d\u4e00\u81f4\u800c\u52a0\u8f7d\u5931\u8d25\u3002</p>"},{"location":"UNet/#cropandconcat-class","title":"<code>CropAndConcat</code> Class","text":"<p><code>CropAndConcat</code> \u7c7b\u7684\u4f5c\u7528\u662f\u5c06 Contracting Path \u4e2d\u7684\u7279\u5f81\u56fe\u4e0e Expanding Path \u4e2d\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u62fc\u63a5\uff0c\u4ee5\u4fdd\u8bc1 Expanding Path \u4e2d\u7684\u6bcf\u4e00\u5c42\u90fd\u80fd\u591f\u5229\u7528 Contracting Path \u4e2d\u7684\u4fe1\u606f\u3002</p> <ul> <li>\u9700\u8981\u4f7f\u7528 <code>torchvision.transforms.functional.center_crop(...)</code> \u5bf9 Contracting Path \u4e2d\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u88c1\u526a\uff0c\u4ee5\u4fdd\u8bc1\u5c3a\u5bf8\u4e00\u81f4\u80fd\u591f\u6210\u529f\u62fc\u63a5</li> <li><code>b, c, h, w</code> \u56db\u4e2a\u53d8\u91cf\u5e76\u4e0d\u662f\u90fd\u4f1a\u7528\u5230\uff0c\u4f46\u662f\u4f60\u9700\u8981\u77e5\u9053\u5b83\u4eec\u7684\u542b\u4e49\uff0c\u4fbf\u4e8e\u6b63\u786e\u4f7f\u7528 <code>center_crop(...)</code><ul> <li><code>b</code>\uff1abatch size</li> <li><code>c</code>\uff1achannel</li> <li><code>h</code>\uff1aheight</li> <li><code>w</code>\uff1awidth</li> </ul> </li> <li>\u8bf7\u81ea\u884c\u641c\u7d22\u67e5\u627e <code>torch.cat()</code> \u7684\u7528\u6cd5\uff0c\u4f7f\u5f97\u80fd\u7b26\u5408\u539f\u8bba\u6587\u4e2d\u7684\u62fc\u63a5\u65b9\u5f0f</li> <li>\u4e0e\u56fe\u4e2d\u76f8\u53cd\uff0c\u5b9e\u9645\u62fc\u63a5\u7684\u987a\u5e8f\u4e3a Expanding Path \u4e2d\u7684 feature map \u5728\u5de6\uff0cContracting Path \u4e2d\u7684feature map \u5728\u53f3</li> <li>\u8fd9\u91cc\u7684\u4ee3\u7801\u91cf\u975e\u5e38\u5c0f\uff0c\u57fa\u672c\u4e24\u4e09\u884c\uff0c\u4e0d\u8981\u5199\u590d\u6742\u4e86</li> </ul> <pre><code>class CropAndConcat(nn.Module):\n    \"\"\"\n    ### Crop and Concatenate the feature map\n\n    Crop the feature map from the contracting path to the size of the current feature map\n    \"\"\"\n    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n        \"\"\"\n        :param x: current feature map in the expansive path\n        :param contracting_x: corresponding feature map from the contracting path\n        \"\"\"\n\n        b, c, h, w = x.shape\n\n        # TODO: Concatenate the feature maps\n        # use torchvision.transforms.functional.center_crop(...)\n        x = torch.cat(\n            # ...\n        )\n\n        return x\n</code></pre>"},{"location":"UNet/#forward-in-unet-class","title":"<code>forward</code> in <code>UNet</code> Class","text":"<p>\u524d\u9762\u5982\u679c\u90fd\u5b9e\u73b0\u6b63\u786e\uff0c\u8fd9\u91cc\u662f\u6bd4\u8f83\u7b80\u5355\u7684\uff0c\u76f8\u5f53\u4e8e\u5bf9\u7740\u56fe\u8fde\u7ebf\u3002\u6ce8\u610f\u5728 Contracting Path \u4e2d\u4fdd\u7559\u4e2d\u95f4\u7ed3\u679c\uff0c\u5728 Expanding Path \u4e2d Crop and Concat \u65f6\u53ef\u4ee5\u4f7f\u7528\u3002</p>"},{"location":"UNet/#test-the-network-completion","title":"Test the Network Completion","text":"<p>\u63d0\u4f9b\u6a21\u578b\u6587\u4ef6 <code>model.pth</code>\uff0c\u4f60\u53ef\u4ee5\u7528\u4ee5\u4e0b\u7684\u4ee3\u7801\u6d4b\u8bd5\u4f60\u8865\u5168\u7684\u7f51\u7edc\u662f\u5426\u80fd\u6210\u529f\u52a0\u8f7d\u8be5\u6a21\u578b\u3002</p> <pre><code>import argparse\nimport torch\nfrom unet import UNet\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Predict masks from input images')\n    parser.add_argument('--model', '-m', default='model.pth',\n                        help='Specify the file in which the model is stored')\n    args = parser.parse_args()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    print(f'Loading model {args.model}')\n    print(f'Using device {device}')\n\n    model = UNet(in_channels=3, out_channels=1).to(device)\n    state_dict = torch.load(args.model, map_location=device)\n    model.load_state_dict(state_dict)\n\n    print('Model loaded')\n</code></pre> <p>\u4f8b\u5982\uff0c\u5c06\u8be5\u6d4b\u8bd5\u4ee3\u7801\u5199\u5728 <code>try.py</code>\uff0c\u5e76\u5c06\u5176\u548c <code>unet.py</code> \u653e\u5728\u540c\u4e00\u76ee\u5f55\u4e0b\uff0c\u7528 <code>--model</code> \u6307\u5b9a <code>model.pth</code> \u7684\u8def\u5f84\uff0c\u4ee5\u6a21\u578b\u6587\u4ef6\u548c\u4ee3\u7801\u5728\u540c\u4e00\u76ee\u5f55\u4e3a\u4f8b\uff0c\u5219\u53ef\u4ee5\u8fd0\u884c</p> <pre><code>python try.py --model model.pth\n</code></pre> <p>\u5982\u679c\u8f93\u51fa <pre><code>Loading model model.pth\nUsing device cpu\nModel loaded\n</code></pre></p> <p>\u5219\u8bf4\u660e\u4f60\u7684\u7f51\u7edc\u8865\u5168\u6b63\u786e\u3002\u5f53\u7136\uff0c\u7b2c\u4e00\u4e8c\u884c\u7684\u8f93\u51fa\u4e0d\u540c\u60c5\u51b5\u53ef\u80fd\u4e0d\u540c\uff0c\u6211\u4eec\u5173\u6ce8\u7684\u91cd\u5fc3\u5728\u4e8e\u7b2c\u4e09\u884c\u8f93\u51fa \"Model loaded\"\u3002</p>"},{"location":"UNet/#inference-for-single-image","title":"Inference for Single Image","text":"<p>\u8981\u6c42\u52a0\u8f7d\u63d0\u4f9b\u7684\u6a21\u578b <code>model.pth</code>\uff0c\u5bf9\u63d0\u4f9b\u7684\u5355\u5f20\u6c7d\u8f66\u56fe\u7247\u7684 mask \u8fdb\u884c\u63a8\u65ad\uff0c\u6709\u5982\u4e0b\u7684\u5173\u952e\u70b9\uff1a</p> <ul> <li>\u4f7f\u7528 <code>Image.open()</code> \u8bfb\u5165\u7684\u5355\u5f20\u56fe\u7247\u9700\u8981\u5229\u7528 <code>torchvision.transforms</code> \u8fdb\u884c\u9002\u5f53\u7684\u9884\u5904\u7406\u3002<ul> <li>Resize \u4e3a 572</li> <li>\u8f6c\u6362\u4e3a Tensor</li> </ul> </li> <li>\u53ef\u80fd\u9700\u8981\u7528\u5230 <code>torch.nn.functional.interpolate</code> \u8fdb\u884c\u63d2\u503c\uff0c\u548c\u56fe\u7247\u5c3a\u5bf8\u5339\u914d</li> <li>\u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u90fd\u5177\u6709 <code>[B, C, H, W]</code> \u7684\u683c\u5f0f</li> <li>\u6a21\u578b\u76f4\u63a5\u4ea7\u751f\u7684\u8f93\u51fa\u662f\u4e00\u4e2a score\u3002\u9996\u5148\u9700\u8981\u7528 sigmoid \u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u4f7f\u7528\u4e00\u5b9a\u7684\u9608\u503c\u6765\u5c06\u5176\u8f6c\u6362\u4e3a 0-1 \u7684 mask</li> </ul> <p>\u5728\u8fd9\u91cc\u7b80\u5355\u63d0\u4f9b\u7ed9\u5b9a\u56fe\u7247 <code>img</code>\u3001\u9884\u6d4b\u7684\u63a9\u7801 <code>mask</code> \u548c\u4fdd\u5b58\u56fe\u7247\u540d <code>filename</code> \u800c\u5c06\u56fe\u7247\u548c\u9884\u6d4b\u7684\u63a9\u7801\u4ee5 <code>filename</code> \u4fdd\u5b58\u7684\u4ee3\u7801\uff1a</p> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_img_and_mask(img, mask, filename):\n    classes = mask.max()\n    fig, ax = plt.subplots(1, classes + 1)\n    ax[0].set_title('Input image')\n    ax[0].imshow(img)\n    ax[1].set_title('Mask')\n    ax[1].imshow(mask == 0)\n    plt.savefig(filename)\n    plt.close()\n</code></pre> <p>\u6548\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p>"},{"location":"UNet/#download-links","title":"Download Links","text":"<ul> <li>unet.py</li> <li>try.py</li> <li>model.pth: \u6682\u7f3a\uff0c\u5982\u6709\u9700\u8981\u53ef\u4ee5\u8054\u7cfb ZhouTimeMachine</li> </ul>"},{"location":"UNet/#references","title":"References","text":"<ul> <li>PyTorch</li> <li>PyTorch Lightning</li> <li>MNIST dataset (verification is needed)</li> <li>LeNet paper Gradient-based learning applied to document recognition</li> <li>PyTorch extending</li> <li>Dive into Deep Learning</li> <li>Carvana dataset</li> <li>U-Net paper U-Net: Convolutional Networks for Biomedical Image Segmentation</li> </ul>"}]}